{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225f2b07-4840-4b1a-bfb9-61295cfbbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we define the tests parameters - here is the only cell where we need to give our input\n",
    "\n",
    "input_file_name = \"10.json\"\n",
    "\n",
    "split_length = 100\n",
    "tokenizer_model = None\n",
    "use_gpu = False\n",
    "\n",
    "Readers = [\n",
    "   \"FARMReader\",\n",
    "    \"TransformersReader\"\n",
    "]\n",
    "\n",
    "reference_test_id = (\"FARMReader\", \"deepset/roberta-base-squad2\", (10,3))\n",
    "\n",
    "Language_Models = [\n",
    "                   \"deepset/minilm-uncased-squad2\"\n",
    "                 ,\"deepset/roberta-base-squad2-distilled\"\n",
    "                 ,\"deepset/roberta-base-squad2\"\n",
    "                 ,\"deepset/electra-base-squad2\"\n",
    "                 ,\"deepset/tinyroberta-6l-768d\"\n",
    "                 ,\"distilbert-base-uncased-distilled-squad\"\n",
    "                 ,\"distilbert-base-uncased\"\n",
    "                 ,\"deepset/bert-base-cased-squad2\"\n",
    "                 ,\"deepset/electra-base-squad2\"\n",
    "                 ,\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "]\n",
    "\n",
    "topK_Retriever_Reader_Scenarios = [\n",
    "   (10,3)\n",
    "#                                  ,(10,5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2189a8-b1cd-4398-b270-21072ac12182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from haystack.nodes import TfidfRetriever\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack.schema import Document\n",
    "from haystack.nodes import PreProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27b0141-984b-460c-aca6-dd81ff69d742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Tests = list(itertools.product(Readers, Language_Models, topK_Retriever_Reader_Scenarios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a139d75-33d1-455c-ba3f-5517d2ad9ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=False,\n",
    "    split_by=\"word\",\n",
    "    split_length=split_length,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccc8dd40-1bc5-43af-9f6b-4bc1b26ff058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    with open(path) as f:\n",
    "        j = json.load(f)\n",
    "        return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bd8020d-40f4-46fb-8e8b-fdbc2ee466ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_doc(path, preprocessor):\n",
    "    data = read_file(path)\n",
    "    docs = [Document(content=t).to_dict() for t in data['texts']]\n",
    "\n",
    "    return preprocessor.process(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe7bcf9-4c14-4fd2-b73d-15d9097c17b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def file_to_docstore(name):\n",
    "    document_store = InMemoryDocumentStore()\n",
    "    \n",
    "    fpath = f'./data/{name}'\n",
    "    docs = []\n",
    "    adi_counter = 0\n",
    "    for d in file_to_doc(fpath, preprocessor):\n",
    "        \n",
    "        adi_counter = adi_counter + 1\n",
    "        \n",
    "        d['id'] = f\"{d['id']}-{d['meta']['_split_id']}\"\n",
    "        \n",
    "        docs.append(d)\n",
    "            \n",
    "    print(f\"{len(docs)} documents found\")\n",
    "    data = read_file(fpath)\n",
    "    \n",
    "    document_store.write_documents(docs)\n",
    "    retriever = TfidfRetriever(document_store=document_store)\n",
    "    \n",
    "    return document_store, retriever, data['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2522d73c-688e-47c7-a9e2-7a3b5635cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(prediction):\n",
    "    scores = [\"{:.3f}\".format(a.score) for a in prediction['answers']]\n",
    "    answers = [a.answer for a in prediction['answers']]\n",
    "    df = pd.DataFrame(dict(scores=scores, answers=answers))\n",
    "    \n",
    "    return df           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fca49df0-a5c7-417c-9b96-35fd0d56fd19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA:0\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1354.97docs/s]\n",
      "INFO - haystack.nodes.retriever.sparse -  Found 34 candidate paragraphs from 34 docs in DB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 documents found\n"
     ]
    }
   ],
   "source": [
    "document_store, retriever, query = file_to_docstore(input_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f2a9b6a-74e3-485d-9997-2bef43d87ec9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_parameters_from_test_id(test_id):\n",
    "    #test_id is a tuple with 3 elements:\n",
    "    #test_id[0] = Reader\n",
    "    #test_id[1] = Language_Model\n",
    "    #test_id[2] = topK_Retriever_Reader_Scenario\n",
    "    #e.g. (\"FARMReader\", \"deepset/minilm-uncased-squad2\", (10,3))\n",
    "    \n",
    "    test_Reader = test_id[0]\n",
    "    test_Language_Model = test_id[1]\n",
    "    test_topK_Retriever_Reader_Scenario = test_id[2]\n",
    "    \n",
    "    return test_Reader, test_Language_Model, test_topK_Retriever_Reader_Scenario            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4674de1f-fec1-4437-a29c-b213e82461c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_test_topK_Retriever_from_test_id(test_id):\n",
    "    \n",
    "    return test_id[2][0]                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3268aa7-254d-47bd-92b9-18c8e5e99a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_test_topK_Reader_from_test_id(test_id):\n",
    "    \n",
    "    return test_id[2][1]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4128a777-3434-47f4-89e2-2ed865a6012e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_reader_from_test_id(test_id):\n",
    "    test_Reader, test_Language_Model, test_topK_Retriever_Reader_Scenario = get_parameters_from_test_id(test_id)\n",
    "    \n",
    "    reader = None\n",
    "    \n",
    "    if (test_Reader == \"FARMReader\"):\n",
    "        reader = FARMReader(model_name_or_path=test_Language_Model, use_gpu=use_gpu)\n",
    "        \n",
    "    if (test_Reader == \"TransformersReader\"):\n",
    "        reader = TransformersReader(model_name_or_path=test_Language_Model, tokenizer=tokenizer_model, use_gpu=use_gpu)\n",
    "        \n",
    "    return reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60b75f9b-63d3-4ca9-acec-1b0ac0782e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_comparison_score_for_same_length_df(refference_df, compared_df):\n",
    "    \n",
    "    prediction_comparison_score = 0\n",
    "    \n",
    "    #we create a set out of each data frame elements from the column \"answers\"\n",
    "    \n",
    "    refference_df_set = set(refference_df[\"answers\"])\n",
    "    compared_df_set = set(compared_df[\"answers\"])\n",
    "    \n",
    "    # we will do an union of the two sets, and we will allocate the score of \n",
    "    # 0.5 * [cardinal(union_set) - ((cardinal(refference_df_set) + cardinal(compared_df_set))]\n",
    "    # then we will compare the elements position by position and \n",
    "    # for each coincidence we will add 0.5 to the prediction_comparison_score\n",
    "    \n",
    "    intersection_set = refference_df_set.intersection(compared_df_set)\n",
    "    \n",
    "    prediction_comparison_score = 0.5 * len(intersection_set)\n",
    "    \n",
    "    for i in range(len(compared_df)):\n",
    "        compared_string = str(compared_df[\"answers\"][i])\n",
    "        reference_string = str(refference_df[\"answers\"][i])\n",
    "\n",
    "        if (compared_string.strip() == reference_string.strip()):\n",
    "            prediction_comparison_score += 0.5\n",
    "        \n",
    "    return prediction_comparison_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07e09950-2d90-434a-8842-e3636315baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_comparison_score(refference_df, compared_df):\n",
    "    \n",
    "    prediction_comparison_score = 0\n",
    "\n",
    "    refference_df_number_of_elements = refference_df.shape[0]\n",
    "    compared_df_number_of_elements = compared_df.shape[0]\n",
    "    \n",
    "    if (compared_df_number_of_elements != refference_df_number_of_elements):\n",
    "        min_number_of_df_elements = min(refference_df_number_of_elements, compared_df_number_of_elements)\n",
    "        \n",
    "        adjusted_refference_df = refference_df.heaf(min_number_of_df_elements)\n",
    "        adjusted_compared_df = compared_df.head(min_number_of_df_elements)\n",
    "        \n",
    "        prediction_comparison_score = get_predictions_comparison_score_for_same_length_df(adjusted_refference_df, adjusted_compared_df)\n",
    "    else:\n",
    "        prediction_comparison_score = get_predictions_comparison_score_for_same_length_df(refference_df, compared_df)\n",
    "    \n",
    "    return prediction_comparison_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4bd0b66-0d03-4b6b-be3f-298c9877212e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def execute_test(test_id):\n",
    "    \n",
    "    reader = get_reader_from_test_id(test_id)\n",
    "    \n",
    "    top_k_Retriever = get_test_topK_Retriever_from_test_id(test_id)\n",
    "    top_k_Reader = get_test_topK_Reader_from_test_id(test_id)\n",
    "    \n",
    "    pipe = ExtractiveQAPipeline(reader, retriever)\n",
    "    \n",
    "    start = time.time()\n",
    "    prediction = pipe.run(query=query, params={\"Retriever\": {\"top_k\": top_k_Retriever}, \"Reader\": {\"top_k\": top_k_Reader}})\n",
    "    end = time.time()\n",
    "    \n",
    "    total_time = end - start\n",
    "    \n",
    "    test_Reader, test_Language_Model, test_topK_Retriever_Reader_Scenario = get_parameters_from_test_id(test_id)\n",
    "    \n",
    "    predictions_df = get_scores(prediction)\n",
    "    \n",
    "    del pipe\n",
    "    pipe = None\n",
    "    \n",
    "    del reader\n",
    "    reader = None\n",
    "\n",
    "    return predictions_df, total_time\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e36f7b44-8d98-4d36-b638-63f5365cda02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find deepset/roberta-base-squad2 locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
      "INFO - haystack.modeling.model.language_model -  Loaded deepset/roberta-base-squad2\n",
      "INFO - haystack.modeling.logger -  ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.infer -  Got ya 15 parallel workers to do inference ...\n",
      "INFO - haystack.modeling.infer -   0     0     0     0     0     0     0     0     0     0     0     0     0     0     0  \n",
      "INFO - haystack.modeling.infer -  /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /|\\  /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /|\\\n",
      "INFO - haystack.modeling.infer -  /'\\   / \\   /'\\   /'\\   / \\   / \\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   / \\   /'\\   /'\\ \n",
      "Inferencing Samples:   0%|                                                                                                                                            | 0/1 [00:00<?, ? Batches/s]/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/haystack/modeling/model/prediction_head.py:485: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  start_indices = flat_sorted_indices // max_seq_len\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.61 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.20 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.19 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.23 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.22 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.03 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.06 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.06 Batches/s]\n",
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find deepset/minilm-uncased-squad2 locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
      "INFO - haystack.modeling.model.language_model -  Loaded deepset/minilm-uncased-squad2\n",
      "INFO - haystack.modeling.logger -  ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.infer -  Got ya 15 parallel workers to do inference ...\n",
      "INFO - haystack.modeling.infer -   0     0     0     0     0     0     0     0     0     0     0     0     0     0     0  \n",
      "INFO - haystack.modeling.infer -  /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /|\\  /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /|\\\n",
      "INFO - haystack.modeling.infer -  /'\\   / \\   /'\\   /'\\   / \\   / \\   /'\\   /'\\   /'\\   /'\\   /'\\   /'\\   / \\   /'\\   /'\\ \n",
      "Inferencing Samples:   0%|                                                                                                                                            | 0/1 [00:00<?, ? Batches/s]/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/haystack/modeling/model/prediction_head.py:485: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  start_indices = flat_sorted_indices // max_seq_len\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18.90 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20.51 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20.41 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.50 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.82 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.95 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.08 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.01 Batches/s]\n",
      "Inferencing Samples: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.41 Batches/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid call for scalar access (setting)!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_715891/468493973.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdict__test_id__prediction_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict__test_id__speed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mexecute_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Speed\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mdict__test_id__prediction_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict__test_id__speed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreference_answers_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_speed_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/py38/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2279\u001b[0m             \u001b[0;31m# GH#33041 fall back to .loc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2280\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2281\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid call for scalar access (setting)!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2283\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid call for scalar access (setting)!"
     ]
    }
   ],
   "source": [
    "#here we execute all the tests\n",
    "\n",
    "import time\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from haystack.nodes import FARMReader, TransformersReader\n",
    "\n",
    "\n",
    "dict__test_id__prediction_df = {}\n",
    "dict__test_id__speed = {}\n",
    "\n",
    "results_df_columns = [\"Speed\", \"Accuracy\", \"Ref_Reader\", \"Ref_LanguageModel\", \"Ref_topK_Retriever\", \"Ref_topK_Reader\"]\n",
    "\n",
    "results_df_rows = []\n",
    "\n",
    "for test in Tests:\n",
    "    results_df_rows.append(test)\n",
    "\n",
    "reference_test_start_time = time.time()\n",
    "reference_answers_df, reference_speed_df = execute_test(reference_test_id)\n",
    "reference_test_end_time = time.time()\n",
    "\n",
    "reference_test_time = reference_test_end_time - reference_test_start_time\n",
    "\n",
    "results_df = pd.DataFrame(columns=results_df_columns, index=results_df_rows)\n",
    "\n",
    "for test in Tests:\n",
    "    if (test != reference_test_id):\n",
    "        start = time.time()\n",
    "        dict__test_id__prediction_df[test], dict__test_id__speed[test]  = execute_test(test)\n",
    "        end = time.time()\n",
    "        results_df.at[test,\"Speed\"] = end - start\n",
    "    else:\n",
    "        dict__test_id__prediction_df[test], dict__test_id__speed[test] = reference_answers_df, reference_speed_df\n",
    "        results_df.at[test,\"Speed\"] = reference_test_time\n",
    "    \n",
    "    accuracy_score = get_predictions_comparison_score(dict__test_id__prediction_df[test], reference_answers_df)\n",
    "    \n",
    "    results_df.at[test,\"Accuracy\"] = accuracy_score\n",
    "    results_df.at[test,\"Ref_Reader\"] = reference_test_id[0]\n",
    "    results_df.at[test,\"Ref_LanguageModel\"] = reference_test_id[1]\n",
    "    results_df.at[test,\"Ref_topK_Retriever\"] = reference_test_id[2][0]\n",
    "    results_df.at[test,\"Ref_topK_Reader\"] = reference_test_id[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614ba4aa-9efe-4d00-890b-47c65b00da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is the results dataframe:\")\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "print(\"Please see below the scatter chart having as points the tests, on X axis the Accuracy and on Y axis the Speed.\")\n",
    "\n",
    "for ticker,row in results_df.iterrows():\n",
    "    plt.scatter(row['Accuracy'], row['Speed'], label=ticker)\n",
    "\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Speed')\n",
    "plt.legend(bbox_to_anchor=(1,1))\n",
    "plt.show()\n",
    "\n",
    "print(\"For reference, please see below the answers and the speed per each test.\")\n",
    "\n",
    "print(\"The reference test answers: \")\n",
    "print(reference_answers_df)\n",
    "      \n",
    "for test in Tests:      \n",
    "    print(\"test is: ==> \" + str(test) + \" <==\")\n",
    "    print(\"answers are: \" + str(dict__test_id__prediction_df[test]))\n",
    "    print(\"speed is: \" + str(dict__test_id__speed[test]))\n",
    "\n",
    "      \n",
    "print(\"\\nDone\")\n",
    "\n",
    "del dict__test_id__prediction_df\n",
    "dict__test_id__prediction_df = None\n",
    "\n",
    "del dict__test_id__speed\n",
    "dict__test_id__speed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e8d425-9fdb-4d4d-b7f6-21bfbd1da859",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_py38",
   "language": "python",
   "name": "conda_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
