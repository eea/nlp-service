{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f145160-04f5-4a52-872f-308ab105406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine-tuning the Model with our own data - by using the tutorial from https://haystack.deepset.ai/docs/latest/tutorial2md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086f385f-88dc-4c09-8c13-f0009325af0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.document_stores.base -  Numba not found, replacing njit() with no-op implementation. Enable it with 'pip install numba'.\n",
      "2022-05-05 21:14:09.313849: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-05 21:14:09.313864: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import FARMReader\n",
    "from haystack.utils import fetch_archive_from_http"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08d9bd50-ba89-4138-b123-b8754f04c648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step1: Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f386e437-d623-4c00-a302-818fa1a7fcb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find distilbert-base-uncased-distilled-squad locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
      "INFO - haystack.modeling.model.language_model -  Loaded distilbert-base-uncased-distilled-squad\n",
      "INFO - haystack.modeling.logger -  ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.infer -  Got ya 11 parallel workers to do inference ...\n",
      "INFO - haystack.modeling.infer -   0     0     0     0     0     0     0     0     0     0     0  \n",
      "INFO - haystack.modeling.infer -  /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /|\\  /w\\   /w\\   /w\\ \n",
      "INFO - haystack.modeling.infer -  /'\\   / \\   /'\\   /'\\   / \\   / \\   /'\\   /'\\   /'\\   /'\\   /'\\ \n",
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "Loading data into the data silo ... \n",
      "              ______\n",
      "               |o  |   !\n",
      "   __          |:`_|---'-.\n",
      "  |__|______.-/ _ \\-----.|       \n",
      " (o)(o)------'\\ _ /     ( )      \n",
      " \n",
      "INFO - haystack.modeling.data_handler.data_silo -  LOADING TRAIN DATA\n",
      "INFO - haystack.modeling.data_handler.data_silo -  ==================\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Loading train set from: data/squad20/dev-v2.0.json \n",
      "INFO - haystack.modeling.data_handler.data_silo -  Got ya 11 parallel workers to convert 1204 dictionaries to pytorch datasets (chunksize = 22)...\n",
      "INFO - haystack.modeling.data_handler.data_silo -   0     0     0     0     0     0     0     0     0     0     0  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /|\\  /w\\   /w\\   /w\\ \n",
      "INFO - haystack.modeling.data_handler.data_silo -  /'\\   / \\   /'\\   /'\\   / \\   / \\   /'\\   /'\\   /'\\   /'\\   /'\\ \n",
      "Preprocessing Dataset data/squad20/dev-v2.0.json: 100%|█| 1204/1204 [00:01<00:00\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  LOADING DEV DATA\n",
      "INFO - haystack.modeling.data_handler.data_silo -  =================\n",
      "INFO - haystack.modeling.data_handler.data_silo -  No dev set is being loaded\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  LOADING TEST DATA\n",
      "INFO - haystack.modeling.data_handler.data_silo -  =================\n",
      "INFO - haystack.modeling.data_handler.data_silo -  No test set is being loaded\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  DATASETS SUMMARY\n",
      "INFO - haystack.modeling.data_handler.data_silo -  ================\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Examples in train: 13600\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Examples in dev  : 0\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Examples in test : 0\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Total examples   : 13600\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "INFO - haystack.modeling.data_handler.data_silo -  Longest sequence length observed after clipping:     256\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Average sequence length after clipping: 174.0468382352941\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Proportion clipped:      0.12830882352941175\n",
      "INFO - haystack.modeling.model.optimization -  Loading optimizer `AdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 1e-05}'\n",
      "INFO - haystack.modeling.model.optimization -  Using scheduler 'get_linear_schedule_with_warmup'\n",
      "INFO - haystack.modeling.model.optimization -  Loading schedule `get_linear_schedule_with_warmup`: '{'num_training_steps': 1360, 'num_warmup_steps': 272}'\n",
      "Train epoch 0/0 (Cur. train loss: 1.3122): 100%|█| 1360/1360 [55:40<00:00,  2.46\n",
      "INFO - haystack.nodes.reader.farm -  Saving reader model to my_model\n"
     ]
    }
   ],
   "source": [
    "reader = FARMReader(model_name_or_path=\"distilbert-base-uncased-distilled-squad\", use_gpu=True)\n",
    "data_dir = \"data/squad20\"\n",
    "# data_dir = \"PATH/TO_YOUR/TRAIN_DATA\"\n",
    "reader.train(data_dir=data_dir, train_filename=\"dev-v2.0.json\", use_gpu=True, n_epochs=1, save_dir=\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8103292-3f68-465b-9fe8-6b2a606c735b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.nodes.reader.farm -  Saving reader model to my_model\n"
     ]
    }
   ],
   "source": [
    "# Saving the model happens automatically at the end of training into the `save_dir` you specified\n",
    "# However, you could also save a reader manually again via:\n",
    "reader.save(directory=\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "954d0dc4-c855-40d7-897a-7882cd73b1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Model found locally at my_model\n",
      "INFO - haystack.modeling.model.language_model -  Loaded my_model\n",
      "INFO - haystack.modeling.model.adaptive_model -  Found files for loading 1 prediction heads\n",
      "WARNING - haystack.modeling.model.prediction_head -  Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"training\": true, \"num_labels\": 2, \"ph_output_type\": \"per_token_squad\", \"model_type\": \"span_classification\", \"label_tensor_name\": \"question_answering_label_ids\", \"label_list\": [\"start_token\", \"end_token\"], \"metric\": \"squad\", \"name\": \"QuestionAnsweringHead\"}\n",
      "INFO - haystack.modeling.model.prediction_head -  Loading prediction head from my_model/prediction_head_0.bin\n",
      "INFO - haystack.modeling.data_handler.processor -  Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n",
      "INFO - haystack.modeling.logger -  ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.infer -  Got ya 11 parallel workers to do inference ...\n",
      "INFO - haystack.modeling.infer -   0     0     0     0     0     0     0     0     0     0     0  \n",
      "INFO - haystack.modeling.infer -  /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /|\\  /w\\   /w\\   /w\\ \n",
      "INFO - haystack.modeling.infer -  /'\\   / \\   /'\\   /'\\   / \\   / \\   /'\\   /'\\   /'\\   /'\\   /'\\ \n"
     ]
    }
   ],
   "source": [
    "# If you want to load it at a later point, just do:\n",
    "new_reader = FARMReader(model_name_or_path=\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "877b1b2f-1ce0-4d7d-87db-96782e4535cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-05 22:24:29--  https://raw.githubusercontent.com/deepset-ai/haystack/master/haystack/utils/augment_squad.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8000::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12480 (12K) [text/plain]\n",
      "Saving to: ‘augment_squad.py’\n",
      "\n",
      "augment_squad.py    100%[===================>]  12,19K  --.-KB/s    in 0,002s  \n",
      "\n",
      "2022-05-05 22:24:30 (7,18 MB/s) - ‘augment_squad.py’ saved [12480/12480]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.utils.import_utils -  Fetching from https://nlp.stanford.edu/data/glove.6B.zip to `data/tutorial2`\n",
      "INFO - haystack.utils.import_utils -  Found data stored in `data/tutorial2`. Delete this first if you really want to fetch new data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-05 22:38:50.445488: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-05 22:38:50.445504: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 271kB/s]\n",
      "Downloading: 100%|███████████████████████████| 420M/420M [05:31<00:00, 1.33MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Traceback (most recent call last):\n",
      "  File \"augment_squad.py\", line 305, in <module>\n",
      "    augment_squad(**vars(args))\n",
      "  File \"augment_squad.py\", line 233, in augment_squad\n",
      "    transformers_model.to(device)\n",
      "  File \"/home/adi/anaconda3/envs/py384/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 899, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/adi/anaconda3/envs/py384/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 570, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/adi/anaconda3/envs/py384/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 570, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/adi/anaconda3/envs/py384/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 570, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/adi/anaconda3/envs/py384/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 593, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/adi/anaconda3/envs/py384/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 897, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "  File \"/home/adi/anaconda3/envs/py384/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 214, in _lazy_init\n",
      "    torch._C._cuda_init()\n",
      "RuntimeError: No CUDA GPUs are available\n"
     ]
    }
   ],
   "source": [
    "# Downloading script\n",
    "!wget https://raw.githubusercontent.com/deepset-ai/haystack/master/haystack/utils/augment_squad.py\n",
    "\n",
    "doc_dir = \"data/tutorial2\"\n",
    "\n",
    "# Downloading smaller glove vector file (only for demonstration purposes)\n",
    "glove_url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "fetch_archive_from_http(url=glove_url, output_dir=doc_dir)\n",
    "\n",
    "# Downloading very small dataset to make tutorial faster (please use a bigger dataset for real use cases)\n",
    "s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/squad_small.json.zip\"\n",
    "fetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n",
    "\n",
    "# Just replace the path with your dataset and adjust the output (also please remove glove path to use bigger glove vector file)\n",
    "!python augment_squad.py --squad_path squad_small.json --output_path augmented_dataset.json --multiplication_factor 2 --glove_path glove.6B.300d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e868ce-98a9-444e-9992-4e9db1640a45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Model found locally at my_model\n",
      "INFO - haystack.modeling.model.language_model -  Loaded my_model\n",
      "INFO - haystack.modeling.model.adaptive_model -  Found files for loading 1 prediction heads\n",
      "WARNING - haystack.modeling.model.prediction_head -  Some unused parameters are passed to the QuestionAnsweringHead. Might not be a problem. Params: {\"training\": true, \"num_labels\": 2, \"ph_output_type\": \"per_token_squad\", \"model_type\": \"span_classification\", \"label_tensor_name\": \"question_answering_label_ids\", \"label_list\": [\"start_token\", \"end_token\"], \"metric\": \"squad\", \"name\": \"QuestionAnsweringHead\"}\n",
      "INFO - haystack.modeling.model.prediction_head -  Loading prediction head from my_model/prediction_head_0.bin\n",
      "INFO - haystack.modeling.data_handler.processor -  Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n",
      "INFO - haystack.modeling.logger -  ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.infer -  Got ya 11 parallel workers to do inference ...\n",
      "INFO - haystack.modeling.infer -   0     0     0     0     0     0     0     0     0     0     0  \n",
      "INFO - haystack.modeling.infer -  /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /|\\  /w\\   /w\\   /w\\ \n",
      "INFO - haystack.modeling.infer -  /'\\   / \\   /'\\   /'\\   / \\   / \\   /'\\   /'\\   /'\\   /'\\   /'\\ \n",
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find huawei-noah/TinyBERT_General_6L_768D locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0617d4d2a68946a4a17456119902902d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/390 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75e040ccebd4a7db5ba49bd8b0e914b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/274M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.model.language_model -  Loaded huawei-noah/TinyBERT_General_6L_768D\n",
      "Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForQuestionAnswering: ['fit_denses.5.bias', 'cls.predictions.bias', 'fit_denses.1.weight', 'fit_denses.6.bias', 'fit_denses.3.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.2.weight', 'fit_denses.5.weight', 'fit_denses.4.weight', 'fit_denses.0.weight', 'fit_denses.3.bias', 'fit_denses.1.bias', 'fit_denses.2.bias', 'fit_denses.6.weight', 'fit_denses.4.bias', 'fit_denses.0.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cff7971f72746b38a8689b2faa97e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.logger -  ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.infer -  Got ya 11 parallel workers to do inference ...\n",
      "INFO - haystack.modeling.infer -   0     0     0     0     0     0     0     0     0     0     0  \n",
      "INFO - haystack.modeling.infer -  /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /w\\   /|\\  /w\\   /w\\   /w\\ \n",
      "INFO - haystack.modeling.infer -  /'\\   / \\   /'\\   /'\\   / \\   / \\   /'\\   /'\\   /'\\   /'\\   /'\\ \n",
      "INFO - haystack.modeling.utils -  Using devices: CPU\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 0\n",
      "INFO - haystack.modeling.data_handler.data_silo -  \n",
      "Loading data into the data silo ... \n",
      "              ______\n",
      "               |o  |   !\n",
      "   __          |:`_|---'-.\n",
      "  |__|______.-/ _ \\-----.|       \n",
      " (o)(o)------'\\ _ /     ( )      \n",
      " \n",
      "INFO - haystack.modeling.data_handler.data_silo -  LOADING TRAIN DATA\n",
      "INFO - haystack.modeling.data_handler.data_silo -  ==================\n",
      "INFO - haystack.modeling.data_handler.data_silo -  Loading train set from: augmented_dataset.json \n",
      "INFO - haystack.modeling.data_handler.processor -   Couldn't find augmented_dataset.json locally. Trying to download ...\n",
      "INFO - haystack.modeling.data_handler.processor -  downloading and extracting file notebooks to dir /home/adi/workspace/nlp-service\n",
      "ERROR - haystack.modeling.data_handler.processor -  Cannot download notebooks. Unknown data source.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'augmented_dataset.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# You can use any pre-trained language model as teacher that uses the same tokenizer as the teacher model.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# The number of the layers in the teacher model also needs to be a multiple of the number of the layers in the student.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m student \u001b[38;5;241m=\u001b[39m FARMReader(model_name_or_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuawei-noah/TinyBERT_General_6L_768D\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mstudent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistil_intermediate_layers_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maugmented_dataset.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m student\u001b[38;5;241m.\u001b[39mdistil_prediction_layer_from(teacher, data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/squad20\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev-v2.0.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m student\u001b[38;5;241m.\u001b[39msave(directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_distilled_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py384/lib/python3.8/site-packages/haystack/nodes/reader/farm.py:634\u001b[0m, in \u001b[0;36mFARMReader.distil_intermediate_layers_from\u001b[0;34m(self, teacher_model, data_dir, train_filename, dev_filename, test_filename, use_gpu, devices, batch_size, n_epochs, learning_rate, max_seq_len, warmup_proportion, dev_split, evaluate_every, save_dir, num_processes, use_amp, checkpoint_root_dir, checkpoint_every, checkpoints_to_keep, caching, cache_path, distillation_loss, temperature, processor)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistil_intermediate_layers_from\u001b[39m(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    551\u001b[0m     teacher_model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFARMReader\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m     processor: Optional[Processor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    576\u001b[0m ):\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;124;03m    The first stage of distillation finetuning as described in the TinyBERT paper:\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;124;03m    https://arxiv.org/pdf/1909.10351.pdf\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;124;03m    :return: None\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_training_procedure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdev_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gpu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarmup_proportion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_proportion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdev_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluate_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluate_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_root_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_root_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoints_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoints_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mteacher_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistillation_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistillation_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtinybert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py384/lib/python3.8/site-packages/haystack/nodes/reader/farm.py:249\u001b[0m, in \u001b[0;36mFARMReader._training_procedure\u001b[0;34m(self, data_dir, train_filename, dev_filename, test_filename, use_gpu, devices, batch_size, n_epochs, learning_rate, max_seq_len, warmup_proportion, dev_split, evaluate_every, save_dir, num_processes, use_amp, checkpoint_root_dir, checkpoint_every, checkpoints_to_keep, teacher_model, teacher_batch_size, caching, cache_path, distillation_loss_weight, distillation_loss, temperature, tinybert, processor)\u001b[0m\n\u001b[1;32m    237\u001b[0m     data_silo \u001b[38;5;241m=\u001b[39m DistillationDataSilo(\n\u001b[1;32m    238\u001b[0m         teacher_model,\n\u001b[1;32m    239\u001b[0m         teacher_batch_size \u001b[38;5;129;01mor\u001b[39;00m batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m         cache_path\u001b[38;5;241m=\u001b[39mcache_path,\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# caching would need too much memory for tinybert distillation so in that case we use the default data silo\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     data_silo \u001b[38;5;241m=\u001b[39m \u001b[43mDataSilo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistributed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_processes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# 3. Create an optimizer and pass the already initialized model\u001b[39;00m\n\u001b[1;32m    259\u001b[0m model, optimizer, lr_schedule \u001b[38;5;241m=\u001b[39m initialize_optimizer(\n\u001b[1;32m    260\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minferencer\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# model=self.inferencer.model,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m     use_amp\u001b[38;5;241m=\u001b[39muse_amp,\n\u001b[1;32m    268\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/py384/lib/python3.8/site-packages/haystack/modeling/data_handler/data_silo.py:104\u001b[0m, in \u001b[0;36mDataSilo.__init__\u001b[0;34m(self, processor, batch_size, eval_batch_size, distributed, automatic_loading, max_multiprocessing_chunksize, max_processes, multiprocessing_strategy, caching, cache_path)\u001b[0m\n\u001b[1;32m     99\u001b[0m         loaded_from_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loaded_from_cache \u001b[38;5;129;01mand\u001b[39;00m automatic_loading:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# In most cases we want to load all data automatically, but in some cases we rather want to do this\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# later or load from dicts instead of file\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py384/lib/python3.8/site-packages/haystack/modeling/data_handler/data_silo.py:224\u001b[0m, in \u001b[0;36mDataSilo._load_data\u001b[0;34m(self, train_dicts, dev_dicts, test_dicts)\u001b[0m\n\u001b[1;32m    222\u001b[0m     train_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mtrain_filename\n\u001b[1;32m    223\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading train set from: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train_file))\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo train set is being loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py384/lib/python3.8/site-packages/haystack/modeling/data_handler/data_silo.py:131\u001b[0m, in \u001b[0;36mDataSilo._get_dataset\u001b[0;34m(self, filename, dicts)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# loading dicts from file (default)\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dicts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     dicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_to_dicts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# shuffle list of dicts here if we later want to have a random dev set splitted from train set\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mtrain_filename) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filename):\n",
      "File \u001b[0;32m~/anaconda3/envs/py384/lib/python3.8/site-packages/haystack/modeling/data_handler/processor.py:483\u001b[0m, in \u001b[0;36mSquadProcessor.file_to_dicts\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfile_to_dicts\u001b[39m(\u001b[38;5;28mself\u001b[39m, file: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m--> 483\u001b[0m     nested_dicts \u001b[38;5;241m=\u001b[39m \u001b[43m_read_squad_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     dicts \u001b[38;5;241m=\u001b[39m [y \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m nested_dicts \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparagraphs\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dicts\n",
      "File \u001b[0;32m~/anaconda3/envs/py384/lib/python3.8/site-packages/haystack/modeling/data_handler/processor.py:2219\u001b[0m, in \u001b[0;36m_read_squad_file\u001b[0;34m(filename, proxies)\u001b[0m\n\u001b[1;32m   2217\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m locally. Trying to download ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2218\u001b[0m     _download_extract_downstream_data(filename, proxies)\n\u001b[0;32m-> 2219\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[1;32m   2220\u001b[0m     input_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(reader)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m input_data\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'augmented_dataset.json'"
     ]
    }
   ],
   "source": [
    "# Loading a fine-tuned model as teacher e.g. \"deepset/​bert-​base-​uncased-​squad2\"\n",
    "teacher = FARMReader(model_name_or_path=\"my_model\", use_gpu=True)\n",
    "\n",
    "# You can use any pre-trained language model as teacher that uses the same tokenizer as the teacher model.\n",
    "# The number of the layers in the teacher model also needs to be a multiple of the number of the layers in the student.\n",
    "student = FARMReader(model_name_or_path=\"huawei-noah/TinyBERT_General_6L_768D\", use_gpu=True)\n",
    "\n",
    "student.distil_intermediate_layers_from(teacher, data_dir=\".\", train_filename=\"augmented_dataset.json\", use_gpu=True)\n",
    "student.distil_prediction_layer_from(teacher, data_dir=\"data/squad20\", train_filename=\"dev-v2.0.json\", use_gpu=True)\n",
    "\n",
    "student.save(directory=\"my_distilled_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b4e4a1-b275-4876-bb2b-77e93e7c503c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py384",
   "language": "python",
   "name": "py384"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
