{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3e8bec-053c-4c5e-8263-8345c4b8c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/adi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "language = ['english']\n",
    "#language = ['en']\n",
    "\n",
    "stop_words = stopwords.words(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3465844a-ceb9-4780-a5c3-599cd784ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def create_positions_dict(text):\n",
    "    \n",
    "    tokens_position = {}\n",
    "    \n",
    "    if (text is None):\n",
    "        return tokens_position\n",
    "    \n",
    "    #remove delimiters\n",
    "    delimiters = \",.!?/&-:; \"\n",
    "    new_text = ' '.join(w for w in re.split(\"[\"+\"\\\\\".join(delimiters)+\"]\", text) if w)\n",
    "    \n",
    "    #get the tokens\n",
    "    tokens = new_text.split()\n",
    "    \n",
    "    curr_position = 0\n",
    "    \n",
    "    while (curr_position < len(tokens)):\n",
    "        if (tokens[curr_position] not in tokens_position):    \n",
    "            tokens_position[tokens[curr_position]] = set()\n",
    "        \n",
    "        tokens_position[tokens[curr_position]].add(curr_position)\n",
    "        \n",
    "        curr_position = curr_position + 1\n",
    "    \n",
    "    return tokens_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab018cf0-b727-48ef-bcc0-f0ebfbbbe30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_consecutive_tokens(set1, set2):\n",
    "    \n",
    "    for elem1 in set1:\n",
    "        if (elem1+1) in set2:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f78f9-66d2-4be2-b100-2da78b66cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_highlighting(text):\n",
    "    if ('<em>' in text) and ('</em>' in text):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac64351-0e5b-45b6-a224-37bbae7432fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highlighted_text(text):\n",
    "    \n",
    "    text = text.replace('<em>', '')\n",
    "    text = text.replace('</em>', '')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bc3f3f-effc-48f6-b01e-2042b4987b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm used in get_sequences(searched_text, original_es_highlight):\n",
    "\n",
    "#Go through each word from original_es_highlight, one by one.\n",
    "\n",
    "#At every step, we need to know which is the begining of the subsequence of tokens from original_es_highlight, \n",
    "#that are consecutive in searched_text, that is ending in the current token from original_es_highlight.\n",
    "\n",
    "#In order to do so, we hold in a variable the index of the token that is at the begining of this subsequence.\n",
    "\n",
    "#When we advance to a new token from original_es_highlight, we need first to check if the token can continue the already found sequence until the current moment\n",
    "#meaning if the current token is marked (with <em> , </em>) and if it is successive in the searched_text.\n",
    "\n",
    "#if the current token is marked and is consecutive to the previous token (we determine that by looking at the precedence of tokens in searched_text),\n",
    "#then we just go on to the next token, the begining of the sequence remains the same.\n",
    "\n",
    "#if not (the token is not highlighted because is not part of searched_text, or, if it is, is not successive to the precedent token,\n",
    "#then it means that the current subsequence ends at the previous token (including the previous token).\n",
    "\n",
    "#in the process we also verify each token from the sequences so that we know if a sequence contains only stop words or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd7fb2-ce4c-4db9-ae06-dc158b5a7850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences(searched_text, original_es_highlight):\n",
    "    \n",
    "    #store a list of tuples (start_seq, end_seq, status) in highlighted_sequences\n",
    "    #a tuple represent the start and the end of a highlighted sequence and if the sequence has only stop words ('r' - from remove) or not ('k' - keep)\n",
    "    highlighted_sequences = []\n",
    "    \n",
    "    if (searched_text is None) or (original_es_highlight is None):\n",
    "        return highlighted_sequences\n",
    "    \n",
    "    searched_tokens_positions = create_positions_dict(searched_text)\n",
    "    \n",
    "    highlighted_tokens = original_es_highlight.split()\n",
    "    \n",
    "    curr_position = 0\n",
    "    \n",
    "    start_seq = 0\n",
    "    end_seq = 0\n",
    "    only_stop_words_in_seq = True\n",
    "    \n",
    "    while (curr_position < len(highlighted_tokens)):\n",
    "        \n",
    "        curr_token = highlighted_tokens[curr_position]\n",
    "            \n",
    "        if (test_highlighting(curr_token)):\n",
    "            dehighlighted_curr_token = get_highlighted_text(curr_token)\n",
    "                \n",
    "            if (curr_position == 0):\n",
    "                if (dehighlighted_curr_token not in stop_words):\n",
    "                    only_stop_words_in_seq = False\n",
    "                curr_position = curr_position + 1\n",
    "            else:\n",
    "                prev_token = highlighted_tokens[curr_position-1]\n",
    "                    \n",
    "                if (test_highlighting(prev_token)):\n",
    "                    \n",
    "                    dehighlighted_prev_token = get_highlighted_text(prev_token)\n",
    "                    \n",
    "                    if (dehighlighted_curr_token in searched_tokens_positions) and (dehighlighted_prev_token in searched_tokens_positions):\n",
    "                        \n",
    "                        curr_token_set = searched_tokens_positions[dehighlighted_curr_token]\n",
    "                        prev_token_set = searched_tokens_positions[dehighlighted_prev_token]\n",
    "                \n",
    "                        if (test_consecutive_tokens(prev_token_set, curr_token_set)):\n",
    "                            curr_position = curr_position + 1\n",
    "                            if (dehighlighted_curr_token not in stop_words):\n",
    "                                only_stop_words_in_seq = False\n",
    "                            \n",
    "                        else:\n",
    "                            end_seq = curr_position - 1\n",
    "                            if (start_seq <= end_seq):\n",
    "                                if(only_stop_words_in_seq):\n",
    "                                    highlighted_sequences.append((start_seq, end_seq,'r'))\n",
    "                                else:\n",
    "                                    highlighted_sequences.append((start_seq, end_seq,'k'))\n",
    "                            \n",
    "                            start_seq = curr_position\n",
    "                            only_stop_words_in_seq = True\n",
    "                            curr_position = curr_position + 1\n",
    "                    else:\n",
    "                        end_seq = curr_position - 1\n",
    "                        if (start_seq <= end_seq):\n",
    "                            if(only_stop_words_in_seq):\n",
    "                                highlighted_sequences.append((start_seq, end_seq,'r'))\n",
    "                            else:\n",
    "                                highlighted_sequences.append((start_seq, end_seq,'k'))\n",
    "\n",
    "                        curr_position = curr_position + 1\n",
    "                        start_seq = curr_position\n",
    "                        only_stop_words_in_seq = True\n",
    "                else:\n",
    "                    start_seq = curr_position\n",
    "                    only_stop_words_in_seq = True\n",
    "                    curr_position = curr_position + 1\n",
    "                    \n",
    "        else:\n",
    "            end_seq = curr_position - 1\n",
    "            if (start_seq <= end_seq):\n",
    "                if(only_stop_words_in_seq):\n",
    "                    highlighted_sequences.append((start_seq, end_seq,'r'))\n",
    "                else:\n",
    "                    highlighted_sequences.append((start_seq, end_seq,'k'))\n",
    "            \n",
    "            curr_position = curr_position + 1\n",
    "            start_seq = curr_position\n",
    "            only_stop_words_in_seq = True\n",
    "    \n",
    "    return highlighted_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d947fab-0acb-42ff-aa6b-68d05047faf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_only_stop_words(tokens, start_seq, end_seq):\n",
    "    #check if a given sequence contains only stop words\n",
    "    \n",
    "    cursor = start_seq\n",
    "    while (cursor <= end_seq):\n",
    "        curr_token = tokens[cursor]\n",
    "        dehighlighted_curr_token = get_highlighted_text(curr_token)\n",
    "        \n",
    "        if (dehighlighted_curr_token not in stop_words):\n",
    "            return False\n",
    "        \n",
    "        cursor = cursor + 1\n",
    "    \n",
    "    return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0356a5-fc55-463f-8aec-a3ff5fa33cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_removable_tags_occurrences(searched_text, original_es_highlight):\n",
    "\n",
    "    sequences = get_sequences(searched_text, original_es_highlight)\n",
    "    \n",
    "    occurrences_of_removable_tags = []\n",
    "    \n",
    "    if (sequences):\n",
    "        nb_of_tags = 0\n",
    "        for seq in sequences:\n",
    "            \n",
    "            if (seq[2] == 'r'):\n",
    "                cursor = 0\n",
    "                while (cursor <= seq[1]-seq[0]):\n",
    "                    occurrences_of_removable_tags.append(nb_of_tags + cursor + 1)                    \n",
    "                    cursor = cursor + 1\n",
    "            \n",
    "            nb_of_tags = nb_of_tags + seq[1] - int(seq[0]) + 1\n",
    "    \n",
    "    return occurrences_of_removable_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d11602-7bd2-4523-af12-c2245a4f8b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nth_occurence(string, sub, replacement, n):\n",
    "    where = [m.start() for m in re.finditer(sub, string)][n-1]\n",
    "    before = string[:where]\n",
    "    after = string[where:]\n",
    "    after = after.replace(sub, replacement, 1)\n",
    "    newString = before + after\n",
    "    \n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8217c7c-7540-4f82-a4ff-05495a3979ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_text(searched_text, original_es_highlight):\n",
    "    processed_text = original_es_highlight\n",
    "    \n",
    "    removable_tags_occurrences = get_removable_tags_occurrences(searched_text, original_es_highlight)\n",
    "    \n",
    "    nb_of_replacements = 0\n",
    "    \n",
    "    if (removable_tags_occurrences):\n",
    "        for occurence in removable_tags_occurrences:\n",
    "            processed_text = replace_nth_occurence(processed_text, '<em>', '', occurence - nb_of_replacements)\n",
    "            processed_text = replace_nth_occurence(processed_text, '</em>', '', occurence - nb_of_replacements)\n",
    "            \n",
    "            nb_of_replacements = nb_of_replacements + 1\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b2305-ff53-4706-8802-e859cbb92503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test1\n",
    "searched_text1 = \"this is the gas emission from agriculture\"\n",
    "original_es_highlight1 = \"\"\"... anything before ... <em>this</em> <em>is</em> <em>the</em> <em>gas</em> <em>emission</em> <em>from</em> <em>agriculture</em> \n",
    "                          ... anything between ... <em>this</em> <em>is</em> a statement about <em>gas</em> <em>emission</em> ... anything between ...\n",
    "                          <em>this</em> <em>is</em> <em>from</em> Romania\"\"\"\n",
    "processed_text1 = get_processed_text(searched_text1, original_es_highlight1)\n",
    "print(processed_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8cafaec-5cba-4a0d-ab7b-a18368e57b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... anything before ... <em>this</em> <em>is</em> <em>the</em> <em>gas</em> <em>emission</em> <em>from</em> <em>agriculture</em> \n",
      "                          ... anything between ... this is a statement about <em>gas</em> <em>emission</em> ... anything between ...\n",
      "                          <em>this</em> <em>is</em> <em>the</em> <em>gas</em> after\n"
     ]
    }
   ],
   "source": [
    "#test2\n",
    "searched_text2 = \"this is the gas emission from agriculture\"\n",
    "original_es_highlight2 = \"\"\"... anything before ... <em>this</em> <em>is</em> <em>the</em> <em>gas</em> <em>emission</em> <em>from</em> <em>agriculture</em> \n",
    "                          ... anything between ... <em>this</em> <em>is</em> a statement about <em>gas</em> <em>emission</em> ... anything between ...\n",
    "                          <em>this</em> <em>is</em> <em>the</em> <em>gas</em> after\"\"\"\n",
    "processed_text2 = get_processed_text(searched_text2, original_es_highlight2)\n",
    "print(processed_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12a1635d-fcaf-4a38-ab44-e457daa7d348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<em>the</em> <em>gas</em> emission... anything between ... \n",
      "this is the <em>agriculture</em> \n",
      "                          ... anything after ...\n"
     ]
    }
   ],
   "source": [
    "#test3\n",
    "searched_text3 = \"this is the gas emission from agriculture\"\n",
    "original_es_highlight3 = \"\"\"<em>the</em> <em>gas</em> <em>emission</em>... anything between ... \n",
    "<em>this</em> <em>is</em> <em>the</em> <em>agriculture</em> \n",
    "                          ... anything after ...\"\"\"\n",
    "processed_text3 = get_processed_text(searched_text3, original_es_highlight3)\n",
    "print(processed_text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a11a80b-af61-4470-bc59-8c4162a1941d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py384",
   "language": "python",
   "name": "py384"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
