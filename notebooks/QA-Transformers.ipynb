{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7979df6a-fe09-465f-bba1-eb84fc0dd542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fname = \"2.json\"\n",
    "split_length = 100\n",
    "model = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "#\"deepset/minilm-uncased-squad2\"\n",
    "# model = \"deepset/roberta-base-squad2-distilled\"\n",
    "# \"deepset/electra-base-squad2\"\n",
    "# \"deepset/tinyroberta-6l-768d\"\n",
    "# \"distilbert-base-uncased-distilled-squad\"\n",
    "tokenizer_model = None\n",
    "# \"distilbert-base-uncased\"\n",
    "# \"deepset/bert-base-cased-squad2\"\n",
    "# \"deepset/electra-base-squad2\"\n",
    "# \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "#\"distilbert-base-uncased\"\n",
    "\n",
    "scenarios = [\n",
    "    {'Retriever': 100, 'Reader': 100},\n",
    "    {'Retriever': 100, 'Reader': 20},\n",
    "    {'Retriever': 100, 'Reader': 10},\n",
    "    {'Retriever': 100, 'Reader': 5},\n",
    "    {'Retriever': 20, 'Reader': 10},\n",
    "    {'Retriever': 20, 'Reader': 5},\n",
    "    {'Retriever': 10, 'Reader': 10},\n",
    "    {'Retriever': 10, 'Reader': 5},\n",
    "    {'Retriever': 5, 'Reader': 5}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2189a8-b1cd-4398-b270-21072ac12182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.model.optimization -  apex not found, won't use it. See https://nvidia.github.io/apex/\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "from haystack.nodes import TfidfRetriever\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack.schema import Document\n",
    "from haystack.nodes import PreProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccc8dd40-1bc5-43af-9f6b-4bc1b26ff058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    with open(path) as f:\n",
    "        j = json.load(f)\n",
    "        return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd8020d-40f4-46fb-8e8b-fdbc2ee466ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_doc(path, preprocessor):\n",
    "    data = read_file(path)\n",
    "    docs = [Document(content=t).to_dict() for t in data['texts']]\n",
    "    \n",
    "    return preprocessor.process(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac9bbb9f-6acc-459f-b22b-44849de58821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=False,\n",
    "    split_by=\"word\",\n",
    "    split_length=split_length,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe7bcf9-4c14-4fd2-b73d-15d9097c17b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def file_to_docstore(name):\n",
    "    document_store = InMemoryDocumentStore()\n",
    "    \n",
    "    fpath = f'./data/{name}'\n",
    "    print(f\"Processing {name}\") \n",
    "    docs = []\n",
    "    for d in file_to_doc(fpath, preprocessor):\n",
    "        d['id'] = f\"{d['id']}-{d['meta']['_split_id']}\"\n",
    "        # print(d['id'])\n",
    "        docs.append(d)\n",
    "    print(f\"{len(docs)} documents found\")\n",
    "    data = read_file(fpath)\n",
    "    document_store.write_documents(docs)\n",
    "    retriever = TfidfRetriever(document_store=document_store)\n",
    "    \n",
    "    return document_store, retriever, data['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2522d73c-688e-47c7-a9e2-7a3b5635cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(prediction):\n",
    "    scores = [\"{:.3f}\".format(a.score) for a in prediction['answers']]\n",
    "    answers = [a.answer for a in prediction['answers']]\n",
    "    df = pd.DataFrame(dict(scores=scores, answers=answers))\n",
    "    return df           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca49df0-a5c7-417c-9b96-35fd0d56fd19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA:0\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                   | 0/10 [00:00<?, ?docs/s]WARNING - haystack.nodes.preprocessor.preprocessor -  One or more sentence found with word count higher than the split length.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 264.50docs/s]\n",
      "INFO - haystack.document_stores.base -  Duplicate Documents: Document with id 'c9835cb59cca32d4b844354a1c678823-0' already exists in index 'document'\n",
      "INFO - haystack.document_stores.base -  Duplicate Documents: Document with id 'c9835cb59cca32d4b844354a1c678823-1' already exists in index 'document'\n",
      "INFO - haystack.document_stores.base -  Duplicate Documents: Document with id 'c9835cb59cca32d4b844354a1c678823-2' already exists in index 'document'\n",
      "INFO - haystack.document_stores.base -  Duplicate Documents: Document with id 'c9835cb59cca32d4b844354a1c678823-3' already exists in index 'document'\n",
      "INFO - haystack.document_stores.base -  Duplicate Documents: Document with id 'c9835cb59cca32d4b844354a1c678823-4' already exists in index 'document'\n",
      "INFO - haystack.document_stores.base -  Duplicate Documents: Document with id 'c9835cb59cca32d4b844354a1c678823-5' already exists in index 'document'\n",
      "INFO - haystack.document_stores.base -  Duplicate Documents: Document with id 'c9835cb59cca32d4b844354a1c678823-6' already exists in index 'document'\n",
      "INFO - haystack.document_stores.base -  Duplicate Documents: Document with id 'c9835cb59cca32d4b844354a1c678823-7' already exists in index 'document'\n",
      "INFO - haystack.document_stores.base -  Duplicate Documents: Document with id 'c9835cb59cca32d4b844354a1c678823-8' already exists in index 'document'\n",
      "INFO - haystack.document_stores.base -  Duplicate Documents: Document with id 'c9835cb59cca32d4b844354a1c678823-9' already exists in index 'document'\n",
      "INFO - haystack.nodes.retriever.sparse -  Found 1874 candidate paragraphs from 204 docs in DB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214 documents found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<haystack.document_stores.memory.InMemoryDocumentStore at 0x7f6cc43b55b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store, retriever, query = file_to_docstore(fname)\n",
    "document_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eea48c8-6ee4-4ec2-b1f9-44c0fe0d0d40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87767961302641bca13cdb236ab43c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fecf47cd00e646f8b5ab638c34c31102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/86.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at sentence-transformers/multi-qa-MiniLM-L6-cos-v1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6803ddf0714e7ab224f509e61db1d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06aaf982be2d44bd85b0a780f3bbe799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf80532031e04e9d99d82fcaec7f85b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c1584638a542fe92da18c228398fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from haystack.nodes import FARMReader, TransformersReader\n",
    "\n",
    "# reader = FARMReader(model_name_or_path=model, use_gpu=True)\n",
    "\n",
    "reader = TransformersReader(model_name_or_path=model, tokenizer=tokenizer_model, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f2a9b6a-74e3-485d-9997-2bef43d87ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "\n",
    "pipe = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02001dd6-2205-4a22-bef4-2078b3385660",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/question_answering.py:189: UserWarning: topk parameter is deprecated, use top_k instead\n",
      "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time:  0.5447385311126709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py:1077: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "prediction = pipe.run(\n",
    "    query=query, params={\"Retriever\": {\"top_k\": 20}, \"Reader\": {\"top_k\": 10}}\n",
    ")\n",
    "end = time.time()\n",
    "print(\"Total time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1af935f-0f7a-4624-b232-e0c570113ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011</td>\n",
       "      <td>CO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011</td>\n",
       "      <td>CO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011</td>\n",
       "      <td>passenger cars and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011</td>\n",
       "      <td>passenger cars and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.011</td>\n",
       "      <td>passenger cars and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.011</td>\n",
       "      <td>passenger cars and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.011</td>\n",
       "      <td>passenger cars and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.011</td>\n",
       "      <td>passenger cars and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.011</td>\n",
       "      <td>passenger cars and</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  scores             answers\n",
       "0  0.500                    \n",
       "1  0.011                 CO2\n",
       "2  0.011                 CO2\n",
       "3  0.011  passenger cars and\n",
       "4  0.011  passenger cars and\n",
       "5  0.011  passenger cars and\n",
       "6  0.011  passenger cars and\n",
       "7  0.011  passenger cars and\n",
       "8  0.011  passenger cars and\n",
       "9  0.011  passenger cars and"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scores(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c98a25da-e0b8-4bbe-90d7-b811ca5b08fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/question_answering.py:189: UserWarning: topk parameter is deprecated, use top_k instead\n",
      "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py:1077: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/question_answering.py:189: UserWarning: topk parameter is deprecated, use top_k instead\n",
      "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py:1077: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/question_answering.py:189: UserWarning: topk parameter is deprecated, use top_k instead\n",
      "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py:1077: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/question_answering.py:189: UserWarning: topk parameter is deprecated, use top_k instead\n",
      "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py:1077: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/question_answering.py:189: UserWarning: topk parameter is deprecated, use top_k instead\n",
      "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py:1077: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/question_answering.py:189: UserWarning: topk parameter is deprecated, use top_k instead\n",
      "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py:1077: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/question_answering.py:189: UserWarning: topk parameter is deprecated, use top_k instead\n",
      "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py:1077: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/question_answering.py:189: UserWarning: topk parameter is deprecated, use top_k instead\n",
      "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py:1077: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/question_answering.py:189: UserWarning: topk parameter is deprecated, use top_k instead\n",
      "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n",
      "/home/tibi/mambaforge/envs/py38/lib/python3.8/site-packages/transformers/pipelines/base.py:1077: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever</th>\n",
       "      <th>Reader</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.463635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>0.437975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.447068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.462767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.101146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.055170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.032903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Retriever  Reader      time\n",
       "0        100     100  0.463635\n",
       "1        100      20  0.437975\n",
       "2        100      10  0.447068\n",
       "3        100       5  0.462767\n",
       "4         20      10  0.101146\n",
       "5         20       5  0.100081\n",
       "6         10      10  0.055170\n",
       "7         10       5  0.053197\n",
       "8          5       5  0.032903"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "bench = {'Retriever': [], 'Reader': [], 'time': []}\n",
    "\n",
    "for scenario in scenarios:\n",
    "    start = time.time()\n",
    "    prediction = pipe.run(\n",
    "        query=query, params={\"Retriever\": {\"top_k\": scenario['Retriever']}, \"Reader\": {\"top_k\": scenario['Reader']}}\n",
    "    )\n",
    "    end = time.time()\n",
    "    \n",
    "    bench['Retriever'].append(scenario['Retriever'])\n",
    "    bench['Reader'].append(scenario['Reader'])\n",
    "    bench['time'].append(end - start)\n",
    "    \n",
    "df = pd.DataFrame(bench)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "244c5c90-63fe-43b5-b9db-5623cc121097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever</th>\n",
       "      <th>Reader</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0.463635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>0.437975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.447068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.462767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.101146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.055170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.032903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Retriever  Reader      time\n",
       "0        100     100  0.463635\n",
       "1        100      20  0.437975\n",
       "2        100      10  0.447068\n",
       "3        100       5  0.462767\n",
       "4         20      10  0.101146\n",
       "5         20       5  0.100081\n",
       "6         10      10  0.055170\n",
       "7         10       5  0.053197\n",
       "8          5       5  0.032903"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3abf118f-46dd-40d8-bc25-b637b60fb228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever</th>\n",
       "      <th>Reader</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Retriever</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.457081</td>\n",
       "      <td>0.999283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reader</th>\n",
       "      <td>0.457081</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.472118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0.999283</td>\n",
       "      <td>0.472118</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Retriever    Reader      time\n",
       "Retriever   1.000000  0.457081  0.999283\n",
       "Reader      0.457081  1.000000  0.472118\n",
       "time        0.999283  0.472118  1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c4d86f3-a9f4-4057-a4dc-8e0b0f06aae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How much have new cars co2 emissions decreased in 2020?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_file(f\"./data/{fname}\")['query']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_py38",
   "language": "python",
   "name": "conda_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
